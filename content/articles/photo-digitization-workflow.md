---
title: "Catalog, Digitize, Preserve Print Photographs and Negatives"
draft: true
---


The problem is one of scope. Progress is slow because you are trying to take too many photos through too many steps at once.

We want to extract as much value as possible from the collection. But not all items in the collection are of the same value, and so we should find a way to prioritize processing the most valuable items, so that at any given time, the highest value items as far along in the processing pipeline as possible. A more efficient approach is to divide and conquer: figure out a processing pipeline that filters out and then processes the most valuable items at each stage.

People are often uncomfortable with an approach like this at first, despite its obvious advantages. Its natural to think that you should somehow be able to move the entire collection through each stage of the processing pipeline. They worry that only some of the items are being processed at any given time, and not, somehow, all the items. I simply point out the obvious: the choice isn't between processing some items or all items, the choice is between processing some items or none. 

Starting with the entire collection, I believe you can quickly get to a more workable set of photos within a few hours by initially identifying and physically isolating a subset of the highest value photos. I think it's uncontroversial to say that the trestle is by far the highest value Brooktondale subject.

The only thing we are trying to do at this stage is identify and physically isolate the highest value photos. Its important to keep in mind what we ARE NOT DOING at this stage::
- Should photos be cataloged? Later, but not now.
- Should photos be reorganized in some way?  Later, but not now.
- If there is one trestle negative in a sheet of unrelated negatives, should it be pulled out from the others? Later, but now now. For now, put the entire sheet with the others, as filtering a single sheet of negatives is a trivial part of later digitization that would essentially happen anyway.

Those types of questions have answers that are easily discoverable when needed, and in fact, it is quite possible that between the time you identify solutions and are prepared to actually implement them, you will need to re-do this work again anyway. Better to do these tasks just-in-time.

*Be ruthless about the goal: identify and physically isolate photos, based on specific criteria that will result in the highest value batch of photos filtered out of the previous batch, starting with the entire collection.* Any activity that doesn't directly serve that goal is a distraction and is not done. This effectively means that you are always waiting as long as possible to do any given task. You move onto the next stage whenever you have a meaningful, workable set of photos to process in the next stage. Should you have a database to store the photo metadata? Worry about that when you actually have data that could benefit from it. How will you store the digital images? You can figure that out once you actually have digital images. Solve the problems you have, not the problems you don't have. 

As items move through this evolving pipeline, you will figure out not only what should be done in each filtered batch, but how large each batch should become before it is processed in the next stage, solving problems as they arise. Initially, you will probably have some intuition of how large batches should become before they are processed in the next step in the process, but if you don't know, you will just have to pick a number that you are confident is low enough. Start with that and adjust over time as you get a better sense of what is optimal. In fact, if this is closely tracked, over time, it will become more and more predictive. But this level of precision is often unnecessary, and should only be done if there is a clear benefit that outweighs the obvious overhead this tracking would add.

You want to always be moving forward, and doing so more efficiently over time. 

Once you complete the filtering process, or your filtering process has yielded a sufficient batch for the next step in the process, the next step should be taken. If the filter ends with a sub optimally small batch, you may decide to perform an additional filter at that stage, or go onto the next stage, whichever makes the most sense. In a given session, set your goal to get to the furthest stage in the process possible with the most amount of photos. 

I would propose actually delaying cataloguing until after digitization, and focus on cataloguing the digitized images, and naturally, doing the cataloging digitally as well. This will eliminate steps and result in a more natural and convenient workflow.

The important thing to recognize is that your data definition will evolve. 

Delaying cataloguing longer has the side effect that you enter that stage more informed, but even then, recognize that data definitions, the standards you adhere to etc., will evolve over time. Mitigate this risk and associated cost by doing less: at any given time do the minimal amount of effort that will yield the most value. If you are unsure about how to approach something, probably better to wait than risk only multiplying the problems to solve. And if something doesn't have a clear payoff, why do it? 

Of course, waiting to catalog until you are in a digital environment also mitigates the cost of drifting metadata standards. Here too, a minimal approach is advised. Spreadsheets are not perfect, nothing is, but they are ubiquitous, standardized tools that many people understand how to use at a pretty decent level. You potentially only need a sharable solution, and several such options are readily available and cost effective. Here again, the downsides of spreadsheets are proportional to the complexity and scope of the metadata. Keep it simple until it absolutely has to get more complex.Most of the value you will get out of metadata can be captured in a surprisingly small subset of the possible data you may want to include. (Remember that the processing pipeline continues with the researcher/consumer of the items, you only need to provide a useful basis for them to perform their own filter on the collection.)



Generally speaking, when in doubt, resist the urge to guess, and instead do less, such as smaller batches (requiring more specific selection criteria), fewer steps with each batch, etc. Even if the batches are smaller than optimal, at least you will keep moving forward. 

Simpler tasks can be combined, or if the order doesn't permit combining, or it doesn't make sense for other reasons, then simply recognize that simpler tasks can usually be done in larger batches. You will likely have 

In the case of the trestle, there will be relatively few of these, so it may actually make sense to expand your filter to include people (which I propose might be a logical next cut), resulting in two new physically isolated sub-collections, which will optimize the physical handling of the every item in the initial collection. Then, you may be able to take the batch of trestle photos as far as the digitization stage.

When you turn your attention to people, presumably more filtering will be needed. Prioriziing

There will be a relatively small number of course, and so, I think that it would not be significantly extra work to simultaneously identify and physically isolate a larger subset, and I propose that people are not only clearly high value, but the longer they go undigitized, the less able we will be to identify people in those photos.
-- 
adam smith
asmith607@gmail.com
